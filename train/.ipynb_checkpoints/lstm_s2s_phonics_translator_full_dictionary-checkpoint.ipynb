{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOWc-gQBf1TA"
   },
   "source": [
    "# Phonics-level recurrent sequence-to-sequence translation model\n",
    "\n",
    "**Author:** Tsz Lung<br>\n",
    "**Date created:** 2021/09/29<br>\n",
    "**Last modified:** 2021/09/29<br>\n",
    "**Description:** phonics-level recurrent sequence-to-sequence translation model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dULejMDf1TF"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates how to implement a basic phonics-level\n",
    "recurrent sequence-to-sequence translation model. We apply it to translating\n",
    "single word with accent into single word with standard phoneme,\n",
    "phoneme-by-phoneme. Note that it is fairly unusual to\n",
    "do phoneme-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "**Summary of the algorithm**\n",
    "\n",
    "- We start with input sequences from a domain (e.g. single word)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. single word).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next phoneme\n",
    "    - Sample the next phoneme using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled phoneme to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL-J-v-af1TH"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2p0kZ6qEf1TH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load phonics dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/cmusphinxdict.json') as f:\n",
    "  phoneme_dictionary = json.load(f)\n",
    "\n",
    "#cmu phoneme list\n",
    "cmu_phoneme = [\n",
    "            'SPACE',\n",
    "            'AA',\n",
    "            'AE',\n",
    "            'AH',\n",
    "            'AO',\n",
    "            'AW',\n",
    "            'AY',\n",
    "            'B',\n",
    "            'CH',\n",
    "            'D',\n",
    "            'DH',\n",
    "            'EH',\n",
    "            'ER',\n",
    "            'EY',\n",
    "            'F',\n",
    "            'G',\n",
    "            'HH',\n",
    "            'IH',\n",
    "            'IY',\n",
    "            'JH',\n",
    "            'K',\n",
    "            'L',\n",
    "            'M',\n",
    "            'N',\n",
    "            'NG',\n",
    "            'OW',\n",
    "            'OY',\n",
    "            'P',\n",
    "            'R',\n",
    "            'S',\n",
    "            'SH',\n",
    "            'T',\n",
    "            'TH',\n",
    "            'UH',\n",
    "            'UW',\n",
    "            'V',\n",
    "            'W',\n",
    "            'Y',\n",
    "            'Z',\n",
    "            'ZH',\n",
    "        ] \n",
    "\n",
    "\n",
    "def word_to_phonics(input_word):\n",
    "    phonics_blending = []\n",
    "    for word in phoneme_dictionary:\n",
    "        if (input_word == word):\n",
    "#             print(input_word)\n",
    "            for phoneme in phoneme_dictionary[word]:\n",
    "                phoneme = re.split(' ',phoneme)\n",
    "                phonics_blending.append(phoneme)\n",
    "       \n",
    "    return phonics_blending\n",
    "\n",
    "def get_phoneme_index(input_phoneme):\n",
    "    phoneme_index = 0\n",
    "    \n",
    "    for phoneme in cmu_phoneme:\n",
    "        if (phoneme.upper() == input_phoneme):\n",
    "            break\n",
    "\n",
    "        phoneme_index += 1\n",
    "    \n",
    "    return phoneme_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': ['AH', 'EY'],\n",
       " \"A'S\": ['EY Z'],\n",
       " 'A.': ['EY'],\n",
       " \"A.'S\": ['EY Z'],\n",
       " 'A.S': ['EY Z'],\n",
       " 'A42128': ['EY F AO R T UW W AH N T UW EY T'],\n",
       " 'AAA': ['T R IH P AH L EY'],\n",
       " 'AABERG': ['AA B ER G'],\n",
       " 'AACHEN': ['AA K AH N'],\n",
       " 'AACHENER': ['AA K AH N ER'],\n",
       " 'AAKER': ['AA K ER'],\n",
       " 'AALSETH': ['AA L S EH TH'],\n",
       " 'AAMODT': ['AA M AH T'],\n",
       " 'AANCOR': ['AA N K AO R'],\n",
       " 'AARDEMA': ['AA R D EH M AH'],\n",
       " 'AARDVARK': ['AA R D V AA R K'],\n",
       " 'AARON': ['EH R AH N'],\n",
       " \"AARON'S\": ['EH R AH N Z'],\n",
       " 'AARONS': ['EH R AH N Z'],\n",
       " 'AARONSON': ['EH R AH N S AH N', 'AA R AH N S AH N'],\n",
       " \"AARONSON'S\": ['EH R AH N S AH N Z', 'AA R AH N S AH N Z'],\n",
       " 'AARTI': ['AA R T IY'],\n",
       " 'AASE': ['AA S'],\n",
       " 'AASEN': ['AA S AH N'],\n",
       " 'AB': ['AE B', 'EY B IY'],\n",
       " 'ABABA': ['AH B AA B AH', 'AA B AH B AH'],\n",
       " 'ABACHA': ['AE B AH K AH'],\n",
       " 'ABACK': ['AH B AE K'],\n",
       " 'ABACO': ['AE B AH K OW'],\n",
       " 'ABACUS': ['AE B AH K AH S'],\n",
       " 'ABAD': ['AH B AA D'],\n",
       " 'ABADAKA': ['AH B AE D AH K AH'],\n",
       " 'ABADI': ['AH B AE D IY'],\n",
       " 'ABADIE': ['AH B AE D IY'],\n",
       " 'ABAIR': ['AH B EH R'],\n",
       " 'ABALKIN': ['AH B AA L K IH N'],\n",
       " 'ABALONE': ['AE B AH L OW N IY'],\n",
       " 'ABALOS': ['AA B AA L OW Z'],\n",
       " 'ABANDON': ['AH B AE N D AH N'],\n",
       " 'ABANDONED': ['AH B AE N D AH N D'],\n",
       " 'ABANDONING': ['AH B AE N D AH N IH NG'],\n",
       " 'ABANDONMENT': ['AH B AE N D AH N M AH N T'],\n",
       " 'ABANDONMENTS': ['AH B AE N D AH N M AH N T S'],\n",
       " 'ABANDONS': ['AH B AE N D AH N Z'],\n",
       " 'ABANTO': ['AH B AE N T OW'],\n",
       " 'ABARCA': ['AH B AA R K AH'],\n",
       " 'ABARE': ['AA B AA R IY'],\n",
       " 'ABASCAL': ['AE B AH S K AH L'],\n",
       " 'ABASH': ['AH B AE SH'],\n",
       " 'ABASHED': ['AH B AE SH T'],\n",
       " 'ABASIA': ['AH B EY ZH Y AH'],\n",
       " 'ABATE': ['AH B EY T'],\n",
       " 'ABATED': ['AH B EY T IH D'],\n",
       " 'ABATEMENT': ['AH B EY T M AH N T'],\n",
       " 'ABATEMENTS': ['AH B EY T M AH N T S'],\n",
       " 'ABATES': ['AH B EY T S'],\n",
       " 'ABATING': ['AH B EY T IH NG'],\n",
       " 'ABBA': ['AE B AH'],\n",
       " 'ABBADO': ['AH B AA D OW'],\n",
       " 'ABBAS': ['AH B AA S'],\n",
       " 'ABBASI': ['AA B AA S IY'],\n",
       " 'ABBATE': ['AA B EY T'],\n",
       " 'ABBATIELLO': ['AA B AA T IY EH L OW'],\n",
       " 'ABBE': ['AE B IY', 'AE B EY'],\n",
       " 'ABBENHAUS': ['AE B AH N HH AW S'],\n",
       " 'ABBETT': ['AH B EH T'],\n",
       " 'ABBEVILLE': ['AE B V IH L'],\n",
       " 'ABBEY': ['AE B IY'],\n",
       " \"ABBEY'S\": ['AE B IY Z'],\n",
       " 'ABBIE': ['AE B IY'],\n",
       " 'ABBITT': ['AE B IH T'],\n",
       " 'ABBOT': ['AE B AH T'],\n",
       " 'ABBOTSTOWN': ['AE B AH T S T AW N'],\n",
       " 'ABBOTT': ['AE B AH T'],\n",
       " \"ABBOTT'S\": ['AE B AH T S'],\n",
       " 'ABBOTTSTOWN': ['AE B AH T S T AW N'],\n",
       " 'ABBOUD': ['AH B UW D', 'AH B AW D'],\n",
       " 'ABBREVIATE': ['AH B R IY V IY EY T'],\n",
       " 'ABBREVIATED': ['AH B R IY V IY EY T AH D', 'AH B R IY V IY EY T IH D'],\n",
       " 'ABBREVIATES': ['AH B R IY V IY EY T S'],\n",
       " 'ABBREVIATING': ['AH B R IY V IY EY T IH NG'],\n",
       " 'ABBREVIATION': ['AH B R IY V IY EY SH AH N'],\n",
       " 'ABBREVIATIONS': ['AH B R IY V IY EY SH AH N Z'],\n",
       " 'ABBRUZZESE': ['AA B R UW T S EY Z IY'],\n",
       " 'ABBS': ['AE B Z'],\n",
       " 'ABBY': ['AE B IY'],\n",
       " 'ABCO': ['AE B K OW'],\n",
       " 'ABCOTEK': ['AE B K OW T EH K'],\n",
       " 'ABDALLA': ['AE B D AE L AH'],\n",
       " 'ABDALLAH': ['AE B D AE L AH'],\n",
       " 'ABDEL': ['AE B D EH L'],\n",
       " 'ABDELLA': ['AE B D EH L AH'],\n",
       " 'ABDICATE': ['AE B D AH K EY T'],\n",
       " 'ABDICATED': ['AE B D AH K EY T AH D'],\n",
       " 'ABDICATES': ['AE B D AH K EY T S'],\n",
       " 'ABDICATING': ['AE B D IH K EY T IH NG'],\n",
       " 'ABDICATION': ['AE B D IH K EY SH AH N'],\n",
       " 'ABDNOR': ['AE B D N ER'],\n",
       " 'ABDO': ['AE B D OW'],\n",
       " 'ABDOLLAH': ['AE B D AA L AH'],\n",
       " 'ABDOMEN': ['AE B D OW M AH N', 'AE B D AH M AH N'],\n",
       " 'ABDOMINAL': ['AE B D AA M AH N AH L', 'AH B D AA M AH N AH L'],\n",
       " 'ABDUCT': ['AE B D AH K T'],\n",
       " 'ABDUCTED': ['AE B D AH K T IH D', 'AH B D AH K T IH D'],\n",
       " 'ABDUCTEE': ['AE B D AH K T IY'],\n",
       " 'ABDUCTEES': ['AE B D AH K T IY Z'],\n",
       " 'ABDUCTING': ['AE B D AH K T IH NG', 'AH B D AH K T IH NG'],\n",
       " 'ABDUCTION': ['AE B D AH K SH AH N', 'AH B D AH K SH AH N'],\n",
       " 'ABDUCTIONS': ['AE B D AH K SH AH N Z', 'AH B D AH K SH AH N Z'],\n",
       " 'ABDUCTOR': ['AE B D AH K T ER', 'AH B D AH K T ER'],\n",
       " 'ABDUCTORS': ['AE B D AH K T ER Z', 'AH B D AH K T ER Z'],\n",
       " 'ABDUCTS': ['AE B D AH K T S'],\n",
       " 'ABDUL': ['AE B D UW L'],\n",
       " 'ABDULAZIZ': ['AE B D UW L AH Z IY Z'],\n",
       " 'ABDULLA': ['AA B D UW L AH'],\n",
       " 'ABDULLAH': ['AE B D AH L AH'],\n",
       " 'ABE': ['EY B'],\n",
       " 'ABED': ['AH B EH D'],\n",
       " 'ABEDI': ['AH B EH D IY'],\n",
       " 'ABEE': ['AH B IY'],\n",
       " 'ABEL': ['EY B AH L'],\n",
       " 'ABELA': ['AA B EH L AH'],\n",
       " 'ABELARD': ['AE B IH L ER D'],\n",
       " 'ABELE': ['AH B EH L'],\n",
       " 'ABELES': ['AH B EH L Z', 'EY B AH L IY Z'],\n",
       " 'ABELL': ['EY B AH L'],\n",
       " 'ABELLA': ['AH B EH L AH'],\n",
       " 'ABELN': ['AE B IH L N'],\n",
       " 'ABELOW': ['AE B AH L OW'],\n",
       " 'ABELS': ['EY B AH L Z'],\n",
       " 'ABELSON': ['AE B IH L S AH N'],\n",
       " 'ABEND': ['AE B EH N D', 'AH B EH N D'],\n",
       " 'ABENDROTH': ['AE B IH N D R AO TH'],\n",
       " 'ABER': ['EY B ER'],\n",
       " 'ABERCROMBIE': ['AE B ER K R AA M B IY'],\n",
       " 'ABERDEEN': ['AE B ER D IY N'],\n",
       " 'ABERFORD': ['EY B ER F ER D'],\n",
       " 'ABERG': ['AE B ER G'],\n",
       " 'ABERLE': ['AE B ER AH L', 'AE B ER L'],\n",
       " 'ABERMIN': ['AE B ER M IH N'],\n",
       " 'ABERNATHY': ['AE B ER N AE TH IY'],\n",
       " 'ABERNETHY': ['AE B ER N EH TH IY'],\n",
       " 'ABERRANT': ['AE B EH R AH N T'],\n",
       " 'ABERRATION': ['AE B ER EY SH AH N'],\n",
       " 'ABERRATIONAL': ['AE B ER EY SH AH N AH L'],\n",
       " 'ABERRATIONS': ['AE B ER EY SH AH N Z'],\n",
       " 'ABERT': ['AE B ER T'],\n",
       " 'ABET': ['AH B EH T'],\n",
       " 'ABETTED': ['AH B EH T IH D'],\n",
       " 'ABETTING': ['AH B EH T IH NG'],\n",
       " 'ABEX': ['EY B EH K S'],\n",
       " 'ABEYANCE': ['AH B EY AH N S'],\n",
       " 'ABEYTA': ['AA B EY T AH'],\n",
       " 'ABHOR': ['AE B HH AO R'],\n",
       " 'ABHORRED': ['AH B HH AO R D'],\n",
       " 'ABHORRENCE': ['AH B HH AO R AH N S'],\n",
       " 'ABHORRENT': ['AE B HH AO R AH N T'],\n",
       " 'ABHORS': ['AH B HH AO R Z'],\n",
       " 'ABIAM': ['EY B IY AH M'],\n",
       " \"ABIAM'S\": ['EY B IY AH M Z'],\n",
       " 'ABID': ['EY B IH D'],\n",
       " 'ABIDE': ['AH B AY D'],\n",
       " 'ABIDED': ['AH B AY D IH D'],\n",
       " 'ABIDES': ['AH B AY D Z'],\n",
       " 'ABIDING': ['AH B AY D IH NG'],\n",
       " 'ABIDJAN': ['AE B IH JH AA N'],\n",
       " 'ABIE': ['AE B IY'],\n",
       " 'ABIGAIL': ['AE B AH G EY L'],\n",
       " 'ABILA': ['AA B IY L AH'],\n",
       " 'ABILENE': ['AE B IH L IY N'],\n",
       " 'ABILITIES': ['AH B IH L AH T IY Z'],\n",
       " 'ABILITY': ['AH B IH L AH T IY'],\n",
       " 'ABIMAEL': ['AE B IH M EY L'],\n",
       " 'ABIMAELS': ['AE B IH M EY L Z'],\n",
       " 'ABINGDON': ['AE B IH NG D AH N'],\n",
       " 'ABINGTON': ['AE B IH NG T AH N'],\n",
       " 'ABIO': ['AA B IY OW'],\n",
       " 'ABIOLA': ['AA B IY OW L AH'],\n",
       " \"ABIOLA'S\": ['AA B IY OW L AH Z'],\n",
       " 'ABIOMED': ['EY B IY AH M EH D'],\n",
       " 'ABIQUIU': ['AH B IH K Y UW'],\n",
       " 'ABITIBI': ['AE B IH T IY B IY'],\n",
       " 'ABITZ': ['AE B IH T S'],\n",
       " 'ABJECT': ['AE B JH EH K T'],\n",
       " 'ABKHAZIA': ['AE B K AA Z Y AH', 'AE B K AE Z Y AH'],\n",
       " 'ABKHAZIAN': ['AE B K AA Z IY AH N',\n",
       "  'AE B K AE Z IY AH N',\n",
       "  'AE B K AA Z Y AH N',\n",
       "  'AE B K AE Z Y AH N'],\n",
       " 'ABKHAZIANS': ['AE B K AA Z IY AH N Z', 'AE B K AE Z IY AH N Z'],\n",
       " 'ABLAZE': ['AH B L EY Z'],\n",
       " 'ABLE': ['EY B AH L'],\n",
       " 'ABLE-BODIED': ['EY B AH L B AA D IY D'],\n",
       " 'ABLED': ['EY B AH L D'],\n",
       " 'ABLER': ['EY B AH L ER', 'EY B L ER'],\n",
       " 'ABLES': ['EY B AH L Z'],\n",
       " 'ABLEST': ['EY B AH L S T', 'EY B L AH S T'],\n",
       " 'ABLOOM': ['AH B L UW M'],\n",
       " 'ABLY': ['EY B L IY'],\n",
       " 'ABNER': ['AE B N ER'],\n",
       " 'ABNEY': ['AE B N IY'],\n",
       " 'ABNORMAL': ['AE B N AO R M AH L'],\n",
       " 'ABNORMALITIES': ['AE B N AO R M AE L AH T IY Z'],\n",
       " 'ABNORMALITY': ['AE B N AO R M AE L AH T IY'],\n",
       " 'ABNORMALLY': ['AE B N AO R M AH L IY'],\n",
       " 'ABO': ['AA B OW'],\n",
       " \"ABO'S\": ['AA B OW Z'],\n",
       " 'ABOARD': ['AH B AO R D'],\n",
       " 'ABODE': ['AH B OW D'],\n",
       " 'ABOHALIMA': ['AE B AH HH AH L IY M AH'],\n",
       " 'ABOLISH': ['AH B AA L IH SH'],\n",
       " 'ABOLISHED': ['AH B AA L IH SH T'],\n",
       " 'ABOLISHES': ['AH B AA L IH SH IH Z'],\n",
       " 'ABOLISHING': ['AH B AA L IH SH IH NG'],\n",
       " 'ABOLITION': ['AE B AH L IH SH AH N'],\n",
       " 'ABOLITIONISM': ['AE B AH L IH SH AH N IH Z AH M'],\n",
       " 'ABOLITIONIST': ['AE B AH L IH SH AH N AH S T'],\n",
       " 'ABOLITIONISTS': ['AE B AH L IH SH AH N AH S T S',\n",
       "  'AE B AH L IH SH AH N AH S'],\n",
       " 'ABOMINABLE': ['AH B AA M AH N AH B AH L'],\n",
       " 'ABOMINATION': ['AH B AA M AH N EY SH AH N'],\n",
       " 'ABOOD': ['AH B UW D'],\n",
       " 'ABOODI': ['AH B UW D IY'],\n",
       " 'ABOR': ['AH B AO R'],\n",
       " 'ABORIGINAL': ['AE B ER IH JH AH N AH L'],\n",
       " 'ABORIGINE': ['AE B ER IH JH AH N IY'],\n",
       " 'ABORIGINES': ['AE B ER IH JH AH N IY Z'],\n",
       " 'ABORN': ['AH B AO R N'],\n",
       " 'ABORT': ['AH B AO R T'],\n",
       " 'ABORTED': ['AH B AO R T IH D'],\n",
       " 'ABORTIFACIENT': ['AH B AO R T AH F EY SH AH N T'],\n",
       " 'ABORTIFACIENTS': ['AH B AO R T AH F EY SH AH N T S'],\n",
       " 'ABORTING': ['AH B AO R T IH NG'],\n",
       " 'ABORTION': ['AH B AO R SH AH N'],\n",
       " 'ABORTIONIST': ['AH B AO R SH AH N IH S T'],\n",
       " 'ABORTIONISTS': ['AH B AO R SH AH N IH S T S', 'AH B AO R SH AH N IH S'],\n",
       " 'ABORTIONS': ['AH B AO R SH AH N Z'],\n",
       " 'ABORTIVE': ['AH B AO R T IH V'],\n",
       " 'ABORTS': ['AH B AO R T S'],\n",
       " 'ABOTT': ['AH B AA T'],\n",
       " 'ABOU': ['AH B UW'],\n",
       " 'ABOUD': ['AA B UW D'],\n",
       " 'ABOUHALIMA': ['AA B UW HH AA L IY M AH'],\n",
       " \"ABOUHALIMA'S\": ['AA B UW HH AA L IY M AH Z'],\n",
       " 'ABOUND': ['AH B AW N D'],\n",
       " 'ABOUNDED': ['AH B AW N D IH D'],\n",
       " 'ABOUNDING': ['AH B AW N D IH NG'],\n",
       " 'ABOUNDS': ['AH B AW N D Z'],\n",
       " 'ABOUT': ['AH B AW T'],\n",
       " \"ABOUT'S\": ['AH B AW T S'],\n",
       " 'ABOVE': ['AH B AH V'],\n",
       " \"ABOVE'S\": ['AH B AH V Z'],\n",
       " 'ABOVEBOARD': ['AH B AH V B AO R D'],\n",
       " 'ABPLANALP': ['AE B P L AH N AE L P'],\n",
       " 'ABRA': ['AA B R AH'],\n",
       " 'ABRACADABRA': ['AE B R AH K AH D AE B R AH'],\n",
       " 'ABRAHAM': ['EY B R AH HH AE M'],\n",
       " 'ABRAHAMIAN': ['AE B R AH HH EY M IY AH N'],\n",
       " 'ABRAHAMS': ['EY B R AH HH AE M Z'],\n",
       " 'ABRAHAMSEN': ['AE B R AH HH AE M S AH N'],\n",
       " 'ABRAHAMSON': ['AH B R AE HH AH M S AH N'],\n",
       " 'ABRAM': ['AH B R AE M'],\n",
       " \"ABRAM'S\": ['EY B R AH M Z'],\n",
       " 'ABRAMCZYK': ['AA B R AH M CH IH K'],\n",
       " 'ABRAMO': ['AA B R AA M OW'],\n",
       " 'ABRAMOVITZ': ['AH B R AA M AH V IH T S'],\n",
       " 'ABRAMOWICZ': ['AH B R AA M AH V IH CH'],\n",
       " 'ABRAMOWITZ': ['AH B R AA M AH W IH T S'],\n",
       " 'ABRAMS': ['EY B R AH M Z'],\n",
       " \"ABRAMS'S\": ['EY B R AH M Z IH Z'],\n",
       " 'ABRAMSON': ['EY B R AH M S AH N'],\n",
       " 'ABRASION': ['AH B R EY ZH AH N'],\n",
       " 'ABRASIONS': ['AH B R EY ZH AH N Z'],\n",
       " 'ABRASIVE': ['AH B R EY S IH V'],\n",
       " 'ABRASIVES': ['AH B R EY S IH V Z'],\n",
       " 'ABRAXA': ['AH B R AE K S AH'],\n",
       " \"ABRAXA'S\": ['AH B R AE K S AH Z'],\n",
       " 'ABRAXAS': ['AH B R AE K S AH Z'],\n",
       " 'ABREAST': ['AH B R EH S T'],\n",
       " 'ABREGO': ['AA B R EH G OW'],\n",
       " 'ABREU': ['AH B R UW'],\n",
       " 'ABRIDGE': ['AH B R IH JH'],\n",
       " 'ABRIDGED': ['AH B R IH JH D'],\n",
       " 'ABRIDGEMENT': ['AH B R IH JH M AH N T'],\n",
       " 'ABRIDGES': ['AH B R IH JH AH Z'],\n",
       " 'ABRIDGING': ['AH B R IH JH IH NG'],\n",
       " 'ABRIL': ['AH B R IH L'],\n",
       " 'ABROAD': ['AH B R AO D'],\n",
       " 'ABROGATE': ['AE B R AH G EY T'],\n",
       " 'ABROGATED': ['AE B R AH G EY T IH D'],\n",
       " 'ABROGATING': ['AE B R AH G EY T IH NG'],\n",
       " 'ABROGATION': ['AE B R AH G EY SH AH N'],\n",
       " 'ABROL': ['AH B R OW L'],\n",
       " 'ABRON': ['AH B R AA N'],\n",
       " 'ABRUPT': ['AH B R AH P T'],\n",
       " 'ABRUPTLY': ['AH B R AH P T L IY'],\n",
       " 'ABRUPTNESS': ['AH B R AH P T N AH S'],\n",
       " 'ABRUTYN': ['EY B R UW T IH N'],\n",
       " 'ABRUZZESE': ['AA B R UW T S EY Z IY'],\n",
       " 'ABRUZZO': ['AA B R UW Z OW'],\n",
       " 'ABS': ['EY B IY EH S', 'AE B Z'],\n",
       " 'ABSALOM': ['AE B S AH L AH M'],\n",
       " 'ABSARAKA': ['AE B S AA R AH K AH'],\n",
       " 'ABSCAM': ['AE B S K AE M'],\n",
       " 'ABSCESS': ['AE B S EH S'],\n",
       " 'ABSCOND': ['AE B S K AA N D'],\n",
       " 'ABSCONDED': ['AE B S K AA N D AH D'],\n",
       " 'ABSCONDING': ['AE B S K AA N D IH NG'],\n",
       " 'ABSCONDS': ['AE B S K AA N D Z'],\n",
       " 'ABSECON': ['AE B S AH K AO N'],\n",
       " 'ABSENCE': ['AE B S AH N S'],\n",
       " 'ABSENCES': ['AE B S AH N S IH Z'],\n",
       " 'ABSENT': ['AE B S AH N T'],\n",
       " 'ABSENTEE': ['AE B S AH N T IY'],\n",
       " 'ABSENTEEISM': ['AE B S AH N T IY IH Z AH M'],\n",
       " 'ABSENTEES': ['AE B S AH N T IY Z'],\n",
       " 'ABSENTIA': ['AE B S EH N SH AH'],\n",
       " 'ABSHER': ['AE B SH ER'],\n",
       " 'ABSHIER': ['AE B SH IY ER'],\n",
       " 'ABSHIRE': ['AE B SH AY R'],\n",
       " 'ABSINTHE': ['AE B S IH N TH'],\n",
       " 'ABSO': ['AE B S OW'],\n",
       " 'ABSOLOM': ['AE B S AH L AH M'],\n",
       " 'ABSOLUT': ['AE B S AH L UW T'],\n",
       " \"ABSOLUT'S\": ['AE B S AH L UW T S'],\n",
       " 'ABSOLUTE': ['AE B S AH L UW T'],\n",
       " 'ABSOLUTELY': ['AE B S AH L UW T L IY'],\n",
       " 'ABSOLUTENESS': ['AE B S AH L UW T N AH S'],\n",
       " 'ABSOLUTES': ['AE B S AH L UW T S'],\n",
       " 'ABSOLUTION': ['AE B S AH L UW SH AH N'],\n",
       " 'ABSOLUTISM': ['AE B S AH L UW T IH Z AH M'],\n",
       " 'ABSOLUTIST': ['AE B S IH L UW T IH S T'],\n",
       " 'ABSOLVE': ['AH B Z AA L V', 'AE B Z AA L V'],\n",
       " 'ABSOLVED': ['AH B Z AA L V D', 'AE B Z AA L V D'],\n",
       " 'ABSOLVES': ['AH B Z AA L V Z', 'AE B Z AA L V Z'],\n",
       " 'ABSOLVING': ['AH B Z AA L V IH NG', 'AE B Z AA L V IH NG'],\n",
       " 'ABSORB': ['AH B Z AO R B'],\n",
       " 'ABSORBED': ['AH B Z AO R B D'],\n",
       " 'ABSORBENCY': ['AH B Z AO R B AH N S IY'],\n",
       " 'ABSORBENT': ['AH B Z AO R B AH N T'],\n",
       " 'ABSORBER': ['AH B Z AO R B ER'],\n",
       " 'ABSORBERS': ['AH B Z AO R B ER Z'],\n",
       " 'ABSORBING': ['AH B Z AO R B IH NG'],\n",
       " 'ABSORBS': ['AH B Z AO R B Z'],\n",
       " 'ABSORPTION': ['AH B Z AO R P SH AH N', 'AH B S AO R P SH AH N'],\n",
       " 'ABSTAIN': ['AH B S T EY N', 'AE B S T EY N'],\n",
       " 'ABSTAINED': ['AH B S T EY N D', 'AE B S T EY N D'],\n",
       " 'ABSTAINING': ['AH B S T EY N IH NG', 'AE B S T EY N IH NG'],\n",
       " 'ABSTENTION': ['AH B S T EH N CH AH N', 'AE B S T EH N CH AH N'],\n",
       " 'ABSTENTIONS': ['AH B S T EH N CH AH N Z', 'AE B S T EH N CH AH N Z'],\n",
       " 'ABSTINENCE': ['AE B S T AH N AH N S'],\n",
       " 'ABSTINENT': ['AE B S T AH N AH N T'],\n",
       " 'ABSTON': ['AE B S T AH N'],\n",
       " 'ABSTRACT': ['AE B S T R AE K T'],\n",
       " 'ABSTRACTED': ['AE B S T R AE K T IH D'],\n",
       " 'ABSTRACTION': ['AE B S T R AE K SH AH N'],\n",
       " 'ABSTRACTIONS': ['AE B S T R AE K SH AH N Z'],\n",
       " 'ABSTRACTS': ['AE B S T R AE K T S'],\n",
       " 'ABSTRUSE': ['AH B S T R UW S'],\n",
       " 'ABSURD': ['AH B S ER D'],\n",
       " 'ABSURDIST': ['AH B S ER D IH S T'],\n",
       " 'ABSURDITIES': ['AH B S ER D AH T IY Z'],\n",
       " 'ABSURDITY': ['AH B S ER D AH T IY'],\n",
       " 'ABSURDLY': ['AH B S ER D L IY'],\n",
       " 'ABT': ['AE B T', 'EY B IY T IY'],\n",
       " 'ABTS': ['AE B T S', 'EY B IY T IY Z', 'EY B IY T IY EH S'],\n",
       " 'ABU': ['AE B UW'],\n",
       " 'ABUDRAHM': ['AH B AH D R AH M'],\n",
       " 'ABUELLAH': ['AH B W EH L AH'],\n",
       " \"ABUELLAH'S\": ['AH B W EH L AH Z'],\n",
       " 'ABULADZE': ['AE B Y UW L AE D Z IY'],\n",
       " 'ABUNDANCE': ['AH B AH N D AH N S'],\n",
       " 'ABUNDANT': ['AH B AH N D AH N T'],\n",
       " 'ABUNDANTLY': ['AH B AH N D AH N T L IY'],\n",
       " 'ABURTO': ['AH B UH R T OW'],\n",
       " \"ABURTO'S\": ['AH B UH R T OW Z'],\n",
       " 'ABUSE': ['AH B Y UW S', 'AH B Y UW Z'],\n",
       " 'ABUSED': ['AH B Y UW Z D'],\n",
       " 'ABUSER': ['AH B Y UW Z ER'],\n",
       " 'ABUSERS': ['AH B Y UW Z ER Z'],\n",
       " 'ABUSES': ['AH B Y UW S IH Z', 'AH B Y UW Z IH Z'],\n",
       " 'ABUSING': ['AH B Y UW Z IH NG'],\n",
       " 'ABUSIVE': ['AH B Y UW S IH V'],\n",
       " 'ABUT': ['AH B AH T'],\n",
       " 'ABUTS': ['AH B AH T S'],\n",
       " 'ABUTTED': ['AH B AH T AH D'],\n",
       " 'ABUTTING': ['AH B AH T IH NG'],\n",
       " 'ABUZZ': ['AH B AH Z'],\n",
       " 'ABYSMAL': ['AH B IH Z M AH L'],\n",
       " 'ABYSMALLY': ['AH B IH Z M AH L IY'],\n",
       " 'ABYSS': ['AH B IH S'],\n",
       " 'ABYSSINIA': ['AE B S IH N IY AH'],\n",
       " 'ABYSSINIAN': ['AE B S IH N IY AH N'],\n",
       " 'ABZUG': ['AE B Z AH G', 'AE B Z UH G'],\n",
       " 'AC': ['EY S IY'],\n",
       " 'ACA': ['AE K AH'],\n",
       " 'ACACIA': ['AH K EY SH AH'],\n",
       " 'ACADEME': ['AE K AH D IY M'],\n",
       " 'ACADEMIA': ['AE K AH D IY M IY AH'],\n",
       " 'ACADEMIC': ['AE K AH D EH M IH K'],\n",
       " 'ACADEMICALLY': ['AE K AH D EH M IH K L IY'],\n",
       " 'ACADEMICIAN': ['AE K AH D AH M IH SH AH N'],\n",
       " 'ACADEMICIANS': ['AE K AH D AH M IH SH AH N Z',\n",
       "  'AH K AE D AH M IH SH AH N Z'],\n",
       " 'ACADEMICS': ['AE K AH D EH M IH K S'],\n",
       " 'ACADEMIES': ['AH K AE D AH M IY Z'],\n",
       " 'ACADEMY': ['AH K AE D AH M IY'],\n",
       " \"ACADEMY'S\": ['AH K AE D AH M IY Z'],\n",
       " 'ACADIA': ['AH K EY D IY AH'],\n",
       " 'ACAMPO': ['AH K AA M P OW'],\n",
       " 'ACAMPORA': ['AH K AE M P ER AH'],\n",
       " 'ACAMPSIA': ['AH K AE M P S Y AH'],\n",
       " 'ACANTHA': ['AA K AA N DH AH'],\n",
       " 'ACAPULCO': ['AE K AH P UH L K OW'],\n",
       " 'ACARY': ['AE K ER IY'],\n",
       " 'ACCARDI': ['AA K AA R D IY'],\n",
       " 'ACCARDO': ['AA K AA R D OW'],\n",
       " 'ACCEDE': ['AE K S IY D'],\n",
       " 'ACCEDED': ['AE K S IY D IH D'],\n",
       " 'ACCEDES': ['AE K S IY D Z'],\n",
       " 'ACCEDING': ['AE K S IY D IH NG'],\n",
       " 'ACCEL': ['AH K S EH L'],\n",
       " 'ACCELERANT': ['AE K S EH L ER AH N T'],\n",
       " 'ACCELERANTS': ['AE K S EH L ER AH N T S'],\n",
       " 'ACCELERATE': ['AE K S EH L ER EY T'],\n",
       " 'ACCELERATED': ['AE K S EH L ER EY T IH D'],\n",
       " 'ACCELERATES': ['AE K S EH L ER EY T S'],\n",
       " 'ACCELERATING': ['AE K S EH L ER EY T IH NG'],\n",
       " 'ACCELERATION': ['AE K S EH L ER EY SH AH N'],\n",
       " 'ACCELERATOR': ['AE K S EH L ER EY T ER'],\n",
       " 'ACCELERATORS': ['AE K S EH L ER EY T ER Z'],\n",
       " 'ACCELEROMETER': ['AE K S EH L ER AA M AH T ER'],\n",
       " 'ACCELEROMETERS': ['AE K S EH L ER AA M AH T ER Z'],\n",
       " 'ACCENT': ['AH K S EH N T', 'AE K S EH N T'],\n",
       " 'ACCENTED': ['AE K S EH N T IH D'],\n",
       " 'ACCENTING': ['AE K S EH N T IH NG'],\n",
       " 'ACCENTS': ['AE K S EH N T S'],\n",
       " 'ACCENTUATE': ['AE K S EH N CH UW EY T'],\n",
       " 'ACCENTUATED': ['AE K S EH N CH AH W EY T IH D'],\n",
       " 'ACCENTUATES': ['AE K S EH N CH UW EY T S'],\n",
       " 'ACCENTUATING': ['AE K S EH N CH AH W EY T IH NG'],\n",
       " 'ACCEPT': ['AE K S EH P T', 'AH K S EH P T'],\n",
       " 'ACCEPTABILITY': ['AH K S EH P T AH B IH L AH T IY'],\n",
       " 'ACCEPTABLE': ['AE K S EH P T AH B AH L', 'AH K S EH P T AH B AH L'],\n",
       " 'ACCEPTABLY': ['AE K S EH P T AH B L IY', 'AH K S EH P T AH B L IY'],\n",
       " 'ACCEPTANCE': ['AE K S EH P T AH N S', 'AH K S EH P T AH N S'],\n",
       " 'ACCEPTANCES': ['AE K S EH P T AH N S IH Z'],\n",
       " 'ACCEPTED': ['AE K S EH P T IH D', 'AH K S EH P T AH D'],\n",
       " 'ACCEPTING': ['AE K S EH P T IH NG', 'AH K S EH P T IH NG'],\n",
       " 'ACCEPTS': ['AE K S EH P T S'],\n",
       " 'ACCESS': ['AE K S EH S'],\n",
       " 'ACCESSED': ['AE K S EH S T'],\n",
       " 'ACCESSIBILITY': ['AE K S EH S AH B IH L IH T IY'],\n",
       " 'ACCESSIBLE': ['AE K S EH S AH B AH L'],\n",
       " 'ACCESSING': ['AE K S EH S IH NG'],\n",
       " 'ACCESSION': ['AH K S EH SH AH N'],\n",
       " 'ACCESSORIES': ['AE K S EH S ER IY Z'],\n",
       " 'ACCESSORIZE': ['AE K S EH S ER AY Z'],\n",
       " 'ACCESSORIZED': ['AE K S EH S ER AY Z D'],\n",
       " 'ACCESSORY': ['AE K S EH S ER IY'],\n",
       " 'ACCETTA': ['AA CH EH T AH'],\n",
       " 'ACCIDENT': ['AE K S AH D AH N T'],\n",
       " \"ACCIDENT'S\": ['AE K S AH D AH N T S'],\n",
       " 'ACCIDENTAL': ['AE K S AH D EH N T AH L', 'AE K S AH D EH N AH L'],\n",
       " 'ACCIDENTALLY': ['AE K S AH D EH N T AH L IY', 'AE K S AH D EH N AH L IY'],\n",
       " 'ACCIDENTLY': ['AE K S AH D AH N T L IY'],\n",
       " 'ACCIDENTS': ['AE K S AH D AH N T S'],\n",
       " 'ACCION': ['AE CH IY AH N'],\n",
       " 'ACCIVAL': ['AE S IH V AA L'],\n",
       " 'ACCLAIM': ['AH K L EY M'],\n",
       " \"ACCLAIM'S\": ['AH K L EY M Z'],\n",
       " 'ACCLAIMED': ['AH K L EY M D'],\n",
       " 'ACCLAIMING': ['AH K L EY M IH NG'],\n",
       " 'ACCLAIMS': ['AH K L EY M Z'],\n",
       " 'ACCLAMATION': ['AE K L AH M EY SH AH N'],\n",
       " 'ACCLIMATE': ['AE K L AH M EY T'],\n",
       " 'ACCLIMATED': ['AE K L AH M EY T IH D'],\n",
       " 'ACCLIMATION': ['AE K L AH M EY SH AH N'],\n",
       " 'ACCO': ['AE K OW'],\n",
       " 'ACCOKEEK': ['AE K OW K IY K'],\n",
       " 'ACCOLA': ['AA K OW L AH'],\n",
       " 'ACCOLADE': ['AE K AH L EY D'],\n",
       " 'ACCOLADES': ['AE K AH L EY D Z'],\n",
       " 'ACCOMANDO': ['AA K OW M AA N D OW'],\n",
       " 'ACCOMMODATE': ['AH K AA M AH D EY T'],\n",
       " 'ACCOMMODATED': ['AH K AA M AH D EY T AH D'],\n",
       " 'ACCOMMODATES': ['AH K AA M AH D EY T S'],\n",
       " 'ACCOMMODATING': ['AH K AA M AH D EY T IH NG'],\n",
       " 'ACCOMMODATION': ['AH K AA M AH D EY SH AH N'],\n",
       " 'ACCOMMODATIONS': ['AH K AA M AH D EY SH AH N Z'],\n",
       " 'ACCOMMODATIVE': ['AH K AA M AH D EY T IH V'],\n",
       " 'ACCOMPANIED': ['AH K AH M P AH N IY D'],\n",
       " 'ACCOMPANIES': ['AH K AH M P AH N IY Z'],\n",
       " 'ACCOMPANIMENT': ['AH K AH M P N IH M AH N T', 'AH K AH M P N IY M AH N T'],\n",
       " 'ACCOMPANIMENTS': ['AH K AH M P N IH M AH N T S',\n",
       "  'AH K AH M P N IY M AH N T S'],\n",
       " 'ACCOMPANIST': ['AH K AH M P AH N AH S T'],\n",
       " 'ACCOMPANY': ['AH K AH M P AH N IY'],\n",
       " 'ACCOMPANYING': ['AH K AH M P AH N IY IH NG'],\n",
       " 'ACCOMPLI': ['AA K AA M P L IY', 'AH K AA M P L IY'],\n",
       " 'ACCOMPLICE': ['AH K AA M P L AH S'],\n",
       " 'ACCOMPLICES': ['AH K AA M P L AH S AH Z'],\n",
       " 'ACCOMPLISH': ['AH K AA M P L IH SH'],\n",
       " 'ACCOMPLISHED': ['AH K AA M P L IH SH T'],\n",
       " 'ACCOMPLISHES': ['AH K AA M P L IH SH IH Z'],\n",
       " 'ACCOMPLISHING': ['AH K AA M P L IH SH IH NG'],\n",
       " 'ACCOMPLISHMENT': ['AH K AA M P L IH SH M AH N T'],\n",
       " 'ACCOMPLISHMENTS': ['AH K AA M P L IH SH M AH N T S'],\n",
       " 'ACCOR': ['AE K AO R'],\n",
       " \"ACCOR'S\": ['AE K ER Z'],\n",
       " 'ACCORD': ['AH K AO R D'],\n",
       " \"ACCORD'S\": ['AH K AO R D Z'],\n",
       " 'ACCORDANCE': ['AH K AO R D AH N S'],\n",
       " 'ACCORDED': ['AH K AO R D IH D'],\n",
       " 'ACCORDING': ['AH K AO R D IH NG'],\n",
       " 'ACCORDINGLY': ['AH K AO R D IH NG L IY'],\n",
       " 'ACCORDION': ['AH K AO R D IY AH N'],\n",
       " 'ACCORDIONS': ['AH K AO R D IY AH N Z'],\n",
       " 'ACCORDS': ['AH K AO R D Z'],\n",
       " 'ACCOST': ['AH K AO S T'],\n",
       " 'ACCOSTED': ['AH K AA S T AH D'],\n",
       " 'ACCOSTING': ['AH K AA S T IH NG'],\n",
       " 'ACCOUNT': ['AH K AW N T'],\n",
       " \"ACCOUNT'S\": ['AH K AW N T S'],\n",
       " 'ACCOUNTABILITY': ['AH K AW N T AH B IH L IH T IY',\n",
       "  'AH K AW N AH B IH L IH T IY'],\n",
       " 'ACCOUNTABLE': ['AH K AW N T AH B AH L', 'AH K AW N AH B AH L'],\n",
       " 'ACCOUNTANCY': ['AH K AW N T AH N S IY'],\n",
       " 'ACCOUNTANT': ['AH K AW N T AH N T'],\n",
       " \"ACCOUNTANT'S\": ['AH K AW N T AH N T S'],\n",
       " 'ACCOUNTANTS': ['AH K AW N T AH N T S'],\n",
       " \"ACCOUNTANTS'\": ['AH K AW N T AH N T S'],\n",
       " 'ACCOUNTED': ['AH K AW N T AH D', 'AH K AW N AH D'],\n",
       " 'ACCOUNTEMP': ['AH K AW N T EH M P'],\n",
       " 'ACCOUNTEMPS': ['AH K AW N T EH M P S'],\n",
       " 'ACCOUNTING': ['AH K AW N T IH NG', 'AH K AW N IH NG'],\n",
       " 'ACCOUNTS': ['AH K AW N T S'],\n",
       " 'ACCOUTERMENT': ['AH K UW T ER M AH N T'],\n",
       " 'ACCOUTERMENTS': ['AH K UW T ER M AH N T S'],\n",
       " 'ACCOUTREMENT': ['AH K UW T ER M AH N T'],\n",
       " 'ACCOUTREMENTS': ['AH K UW T ER M AH N T S'],\n",
       " 'ACCREDIT': ['AH K R EH D AH T'],\n",
       " 'ACCREDITATION': ['AH K R EH D AH T EY SH AH N'],\n",
       " 'ACCREDITATIONS': ['AH K R EH D AH D EY SH AH N Z'],\n",
       " 'ACCREDITED': ['AH K R EH D IH T IH D'],\n",
       " 'ACCREDITING': ['AH K R EH D AH T IH NG'],\n",
       " 'ACCRETED': ['AH K R IY T IH D'],\n",
       " 'ACCRETION': ['AH K R IY SH AH N'],\n",
       " 'ACCRUAL': ['AH K R UW AH L'],\n",
       " 'ACCRUALS': ['AH K R UW AH L Z'],\n",
       " 'ACCRUE': ['AH K R UW'],\n",
       " 'ACCRUED': ['AH K R UW D'],\n",
       " 'ACCRUES': ['AH K R UW Z'],\n",
       " 'ACCRUING': ['AH K R UW IH NG'],\n",
       " 'ACCU': ['AE K Y UW'],\n",
       " 'ACCUHEALTH': ['AE K Y UW HH EH L TH'],\n",
       " 'ACCUMULATE': ['AH K Y UW M Y AH L EY T'],\n",
       " 'ACCUMULATED': ['AH K Y UW M Y AH L EY T IH D'],\n",
       " 'ACCUMULATES': ['AH K Y UW M Y AH L EY T S'],\n",
       " 'ACCUMULATING': ['AH K Y UW M Y AH L EY T IH NG'],\n",
       " 'ACCUMULATION': ['AH K Y UW M Y AH L EY SH AH N'],\n",
       " 'ACCUMULATIONS': ['AH K Y UW M Y AH L EY SH AH N Z'],\n",
       " 'ACCUMULATIVE': ['AH K Y UW M Y AH L EY T IH V'],\n",
       " 'ACCUMULATIVELY': ['AH K Y UW M Y AH L EY T IH V L IY',\n",
       "  'AH K Y UW M Y AH L AH T IH V L IY'],\n",
       " 'ACCUMULATOR': ['AH K Y UW M Y AH L EY T ER'],\n",
       " 'ACCUMULATORS': ['AH K Y UW M Y AH L EY T ER Z'],\n",
       " 'ACCURACIES': ['AE K Y ER AH S IY Z'],\n",
       " 'ACCURACY': ['AE K Y ER AH S IY'],\n",
       " 'ACCURATE': ['AE K Y ER AH T'],\n",
       " 'ACCURATELY': ['AE K Y ER AH T L IY'],\n",
       " 'ACCURAY': ['AE K Y ER EY'],\n",
       " \"ACCURAY'S\": ['AE K Y ER EY Z'],\n",
       " 'ACCURIDE': ['AE K Y ER AY D'],\n",
       " 'ACCURSO': ['AA K UH R S OW'],\n",
       " 'ACCUSATION': ['AE K Y AH Z EY SH AH N', 'AE K Y UW Z EY SH AH N'],\n",
       " 'ACCUSATIONS': ['AE K Y AH Z EY SH AH N Z', 'AE K Y UW Z EY SH AH N Z'],\n",
       " 'ACCUSATIVE': ['AH K Y UW Z AH T IH V'],\n",
       " 'ACCUSATORY': ['AH K Y UW Z AH T AO R IY'],\n",
       " 'ACCUSE': ['AH K Y UW Z'],\n",
       " 'ACCUSED': ['AH K Y UW Z D'],\n",
       " 'ACCUSER': ['AH K Y UW Z ER'],\n",
       " 'ACCUSERS': ['AH K Y UW Z ER Z'],\n",
       " 'ACCUSES': ['AH K Y UW Z IH Z'],\n",
       " 'ACCUSING': ['AH K Y UW Z IH NG'],\n",
       " 'ACCUSINGLY': ['AH K Y UW Z IH NG L IY'],\n",
       " 'ACCUSTOM': ['AH K AH S T AH M'],\n",
       " 'ACCUSTOMED': ['AH K AH S T AH M D'],\n",
       " 'ACCUTANE': ['AE K Y UW T EY N'],\n",
       " 'ACE': ['EY S'],\n",
       " 'ACECOMM': ['EY S K AA M'],\n",
       " 'ACED': ['EY S T'],\n",
       " 'ACER': ['EY S ER'],\n",
       " 'ACERBIC': ['AH S EH R B IH K'],\n",
       " 'ACERO': ['AH S EH R OW', 'AH S Y EH R OW', 'AH TH EH R OW'],\n",
       " 'ACERRA': ['AH S EH R AH'],\n",
       " 'ACES': ['EY S IH Z'],\n",
       " 'ACETAMINOPHEN': ['AH S IY T AH M IH N AH F AH N'],\n",
       " 'ACETATE': ['AE S AH T EY T'],\n",
       " 'ACETIC': ['AH S EH T IH K', 'AH S IY T IH K'],\n",
       " 'ACETO': ['AA S EH T OW'],\n",
       " 'ACETOCHLOR': ['AA S EH T OW K L AO R'],\n",
       " 'ACETONE': ['AE S AH T OW N'],\n",
       " 'ACETOSYRINGONE': ['AH S EH T OW S IH R IH NG G AO N'],\n",
       " 'ACETYLCHOLINE': ['AH S EH T AH L K OW L IY N', 'AH S IY T AH L K OW L IY N'],\n",
       " 'ACETYLENE': ['AH S EH T AH L IY N'],\n",
       " 'ACEVEDO': ['AE S AH V EY D OW'],\n",
       " 'ACEVES': ['AA S EY V EH S'],\n",
       " 'ACEY': ['EY S IY'],\n",
       " 'ACHAEAN': ['AH CH IY AH N'],\n",
       " 'ACHATZ': ['AE K AH T S'],\n",
       " 'ACHE': ['EY K'],\n",
       " 'ACHEBE': ['AA CH EY B IY'],\n",
       " 'ACHED': ['EY K T'],\n",
       " 'ACHEE': ['AH CH IY'],\n",
       " 'ACHENBACH': ['AE K IH N B AA K'],\n",
       " 'ACHENBAUM': ['AE K AH N B AW M'],\n",
       " 'ACHES': ['EY K S'],\n",
       " 'ACHESON': ['AE CH AH S AH N'],\n",
       " \"ACHESON'S\": ['AE CH AH S AH N Z'],\n",
       " 'ACHESONS': ['AE CH AH S AH N Z'],\n",
       " 'ACHEY': ['AE CH IY'],\n",
       " 'ACHIEVA': ['AH CH IY V AH'],\n",
       " 'ACHIEVABLE': ['AH CH IY V AH B AH L'],\n",
       " 'ACHIEVE': ['AH CH IY V'],\n",
       " 'ACHIEVED': ['AH CH IY V D'],\n",
       " 'ACHIEVEMENT': ['AH CH IY V M AH N T'],\n",
       " 'ACHIEVEMENTS': ['AH CH IY V M AH N T S'],\n",
       " 'ACHIEVER': ['AH CH IY V ER'],\n",
       " 'ACHIEVERS': ['AH CH IY V ER Z'],\n",
       " 'ACHIEVES': ['AH CH IY V Z'],\n",
       " 'ACHIEVING': ['AH CH IY V IH NG'],\n",
       " 'ACHILLE': ['AH K IH L IY'],\n",
       " 'ACHILLES': ['AH K IH L IY Z'],\n",
       " \"ACHILLES'\": ['AH K IH L IY Z'],\n",
       " 'ACHING': ['EY K IH NG'],\n",
       " 'ACHINGLY': ['EY K IH NG L IY'],\n",
       " 'ACHMED': ['AA HH M EH D'],\n",
       " 'ACHOA': ['AH CH OW AH'],\n",
       " \"ACHOA'S\": ['AH CH OW AH Z'],\n",
       " 'ACHOR': ['EY K ER'],\n",
       " 'ACHORD': ['AE K AO R D'],\n",
       " 'ACHORN': ['AE K ER N'],\n",
       " 'ACHTENBERG': ['AE K T EH N B ER G'],\n",
       " 'ACHTERBERG': ['AE K T ER B ER G'],\n",
       " 'ACHY': ['EY K IY'],\n",
       " 'ACID': ['AE S AH D'],\n",
       " 'ACIDIC': ['AH S IH D IH K'],\n",
       " 'ACIDIFICATION': ['AH S IH D AH F AH K EY SH AH N'],\n",
       " 'ACIDIFIED': ['AH S IH D AH F AY D'],\n",
       " 'ACIDIFIES': ['AH S IH D AH F AY Z'],\n",
       " 'ACIDIFY': ['AH S IH D AH F AY'],\n",
       " 'ACIDITY': ['AH S IH D AH T IY'],\n",
       " 'ACIDLY': ['AE S AH D L IY'],\n",
       " 'ACIDOSIS': ['AE S AH D OW S AH S'],\n",
       " 'ACIDS': ['AE S AH D Z'],\n",
       " 'ACIDURIA': ['AE S AH D UH R IY AH'],\n",
       " 'ACIERNO': ['AA S IH R N OW'],\n",
       " 'ACK': ['AE K'],\n",
       " 'ACKER': ['AE K ER'],\n",
       " \"ACKER'S\": ['AE K ER Z'],\n",
       " 'ACKERLEY': ['AE K ER L IY'],\n",
       " 'ACKERLY': ['AE K ER L IY'],\n",
       " 'ACKERMAN': ['AE K ER M AH N'],\n",
       " 'ACKERMANN': ['AE K ER M AH N'],\n",
       " 'ACKERMANVILLE': ['AE K ER M AH N V IH L'],\n",
       " 'ACKERSON': ['AE K ER S AH N'],\n",
       " 'ACKERT': ['AE K ER T'],\n",
       " 'ACKHOUSE': ['AE K HH AW S'],\n",
       " 'ACKLAND': ['AE K L AH N D'],\n",
       " 'ACKLES': ['AE K AH L Z'],\n",
       " 'ACKLEY': ['AE K L IY'],\n",
       " 'ACKLIN': ['AE K L IH N'],\n",
       " 'ACKMAN': ['AE K M AH N'],\n",
       " 'ACKNOWLEDGE': ['AE K N AA L IH JH', 'IH K N AA L IH JH'],\n",
       " 'ACKNOWLEDGEABLE': ['AE K N AA L IH JH AH B AH L',\n",
       "  'IH K N AA L IH JH AH B AH L'],\n",
       " 'ACKNOWLEDGED': ['AE K N AA L IH JH D', 'IH K N AA L IH JH D'],\n",
       " 'ACKNOWLEDGEMENT': ['AE K N AA L IH JH M AH N T',\n",
       "  'IH K N AA L IH JH M AH N T'],\n",
       " 'ACKNOWLEDGEMENTS': ['AE K N AA L IH JH M AH N T S',\n",
       "  'IH K N AA L IH JH M AH N T S'],\n",
       " 'ACKNOWLEDGES': ['AE K N AA L IH JH IH Z', 'IH K N AA L IH JH IH Z'],\n",
       " 'ACKNOWLEDGING': ['AE K N AA L IH JH IH NG', 'IH K N AA L IH JH IH NG'],\n",
       " 'ACKNOWLEDGMENT': ['AE K N AA L IH JH M AH N T',\n",
       "  'IH K N AA L IH JH M AH N T'],\n",
       " 'ACKROYD': ['AE K R OY D'],\n",
       " \"ACKROYD'S\": ['AE K R OY D Z'],\n",
       " 'ACMAT': ['AE K M AE T'],\n",
       " \"ACMAT'S\": ['AE K M AE T S'],\n",
       " 'ACME': ['AE K M IY'],\n",
       " \"ACME'S\": ['AE K M IY Z'],\n",
       " 'ACNE': ['AE K N IY'],\n",
       " 'ACOCELLA': ['AA K OW CH EH L AH'],\n",
       " 'ACOFF': ['AE K AO F'],\n",
       " 'ACOG': ['AH K AO G'],\n",
       " 'ACOLYTE': ['AE K AH L AY T'],\n",
       " 'ACOLYTES': ['AE K AH L AY T S'],\n",
       " 'ACORD': ['AH K AO R D'],\n",
       " 'ACORDIA': ['AH K AO R D IY AH'],\n",
       " 'ACORN': ['EY K AO R N'],\n",
       " \"ACORN'S\": ['EY K AO R N Z'],\n",
       " 'ACORNS': ['EY K AO R N Z'],\n",
       " 'ACOSTA': ['AH K AO S T AH'],\n",
       " 'ACOTT': ['EY K AO T'],\n",
       " 'ACOUSTIC': ['AH K UW S T IH K'],\n",
       " 'ACOUSTICAL': ['AH K UW S T IH K AH L'],\n",
       " 'ACOUSTICALLY': ['AH K UW S T IH K L IY'],\n",
       " 'ACOUSTICS': ['AH K UW S T IH K S'],\n",
       " 'ACQUAINT': ['AH K W EY N T'],\n",
       " 'ACQUAINTANCE': ['AH K W EY N T AH N S'],\n",
       " 'ACQUAINTANCES': ['AH K W EY N T AH N S IH Z'],\n",
       " 'ACQUAINTANCESHIP': ['AH K W EY N T AH N S SH IH P'],\n",
       " 'ACQUAINTED': ['AH K W EY N T IH D', 'AH K W EY N IH D'],\n",
       " 'ACQUAVIVA': ['AA K W AA V IY V AH'],\n",
       " 'ACQUIESCE': ['AE K W IY EH S'],\n",
       " 'ACQUIESCED': ['AE K W IY EH S T'],\n",
       " 'ACQUIESCENCE': ['AE K W IY EH S AH N S'],\n",
       " 'ACQUIESCING': ['AE K W IY EH S IH NG'],\n",
       " 'ACQUIRE': ['AH K W AY ER'],\n",
       " 'ACQUIRED': ['AH K W AY ER D'],\n",
       " 'ACQUIRER': ['AH K W AY ER ER'],\n",
       " \"ACQUIRER'S\": ['AH K W AY ER ER Z'],\n",
       " 'ACQUIRERS': ['AH K W AY ER ER Z'],\n",
       " 'ACQUIRES': ['AH K W AY ER Z'],\n",
       " 'ACQUIRING': ['AH K W AY R IH NG', 'AH K W AY ER IH NG'],\n",
       " 'ACQUISITION': ['AE K W AH Z IH SH AH N'],\n",
       " \"ACQUISITION'S\": ['AE K W AH Z IH SH AH N Z'],\n",
       " 'ACQUISITIONS': ['AE K W AH Z IH SH AH N Z'],\n",
       " 'ACQUISITIVE': ['AH K W IH Z AH T IH V'],\n",
       " 'ACQUIT': ['AH K W IH T'],\n",
       " 'ACQUITAINE': ['AE K W IH T EY N'],\n",
       " 'ACQUITS': ['AH K W IH T S'],\n",
       " 'ACQUITTAL': ['AH K W IH T AH L'],\n",
       " 'ACQUITTALS': ['AH K W IH T AH L Z'],\n",
       " 'ACQUITTED': ['AH K W IH T AH D', 'AH K W IH T IH D'],\n",
       " 'ACQUITTING': ['AH K W IH T IH NG'],\n",
       " 'ACRE': ['EY K ER'],\n",
       " 'ACREAGE': ['EY K ER IH JH', 'EY K R AH JH'],\n",
       " 'ACREE': ['AH K R IY'],\n",
       " 'ACRES': ['EY K ER Z'],\n",
       " 'ACREY': ['AE K R IY'],\n",
       " 'ACRI': ['AA K R IY'],\n",
       " 'ACRID': ['AE K R IH D'],\n",
       " 'ACRIMONIOUS': ['AE K R AH M OW N IY AH S'],\n",
       " 'ACRIMONY': ['AE K R IH M OW N IY'],\n",
       " 'ACRO': ['AE K R OW'],\n",
       " 'ACROBAT': ['AE K R AH B AE T'],\n",
       " 'ACROBATIC': ['AE K R AH B AE T IH K'],\n",
       " 'ACROBATICS': ['AE K R AH B AE T IH K S'],\n",
       " 'ACROBATS': ['AE K R AH B AE T S'],\n",
       " 'ACROLEIN': ['AE K R OW L IY N'],\n",
       " 'ACRONYM': ['AE K R AH N IH M'],\n",
       " 'ACRONYMS': ['AE K R AH N IH M Z'],\n",
       " 'ACROPOLIS': ['AH K R AA P AH L AH S'],\n",
       " 'ACROSS': ['AH K R AO S'],\n",
       " 'ACRYLIC': ['AH K R IH L IH K'],\n",
       " 'ACRYLICS': ['AH K R IH L IH K S'],\n",
       " 'ACT': ['AE K T'],\n",
       " \"ACT'S\": ['AE K T S'],\n",
       " 'ACTAVA': ['AE K T AA V AH'],\n",
       " \"ACTAVA'S\": ['AE K T AA V AH Z'],\n",
       " 'ACTAVAS': ['AE K T AA V AH Z'],\n",
       " 'ACTED': ['AE K T AH D', 'AE K T IH D'],\n",
       " 'ACTEL': ['AE K T EH L'],\n",
       " 'ACTIGALL': ['AE K T IH G AO L'],\n",
       " 'ACTIN': ['AE K T AH N'],\n",
       " 'ACTING': ['AE K T IH NG'],\n",
       " 'ACTINIDE': ['AE K T IH N AY D'],\n",
       " 'ACTINIDIA': ['AE K T IH N IH D IY AH'],\n",
       " 'ACTINOMYCOSIS': ['AE K T IH N OW M AY K OW S IH S'],\n",
       " 'ACTION': ['AE K SH AH N'],\n",
       " \"ACTION'S\": ['AE K SH AH N Z'],\n",
       " 'ACTIONABLE': ['AE K SH AH N AH B AH L'],\n",
       " 'ACTIONS': ['AE K SH AH N Z'],\n",
       " 'ACTIVASE': ['AE K T IH V EY Z'],\n",
       " 'ACTIVATE': ['AE K T AH V EY T'],\n",
       " 'ACTIVATED': ['AE K T AH V EY T AH D', 'AE K T IH V EY T IH D'],\n",
       " 'ACTIVATES': ['AE K T AH V EY T S'],\n",
       " 'ACTIVATING': ['AE K T AH V EY T IH NG'],\n",
       " 'ACTIVATION': ['AE K T AH V EY SH AH N'],\n",
       " 'ACTIVATOR': ['AE K T AH V EY T ER'],\n",
       " 'ACTIVE': ['AE K T IH V'],\n",
       " \"ACTIVE'S\": ['AE K T IH V Z'],\n",
       " 'ACTIVELY': ['AE K T IH V L IY'],\n",
       " 'ACTIVES': ['AE K T IH V Z'],\n",
       " 'ACTIVISION': ['AE K T IH V IH ZH AH N'],\n",
       " 'ACTIVISM': ['AE K T IH V IH Z AH M'],\n",
       " 'ACTIVIST': ['AE K T AH V AH S T', 'AE K T IH V IH S T'],\n",
       " 'ACTIVISTS': ['AE K T AH V AH S T S',\n",
       "  'AE K T IH V IH S T S',\n",
       "  'AE K T AH V AH S',\n",
       "  'AE K T IH V IH S'],\n",
       " \"ACTIVISTS'\": ['AE K T IH V IH S T S', 'AE K T IH V IH S'],\n",
       " 'ACTIVITIES': ['AE K T IH V AH T IY Z', 'AE K T IH V IH T IY Z'],\n",
       " 'ACTIVITY': ['AE K T IH V AH T IY', 'AE K T IH V IH T IY'],\n",
       " 'ACTMEDIA': ['AE K T M IY D IY AH'],\n",
       " 'ACTODINE': ['AE K T OW D AY N'],\n",
       " 'ACTON': ['AE K T AH N'],\n",
       " 'ACTOR': ['AE K T ER'],\n",
       " \"ACTOR'S\": ['AE K T ER Z'],\n",
       " 'ACTORS': ['AE K T ER Z'],\n",
       " \"ACTORS'\": ['AE K T ER Z'],\n",
       " 'ACTRESS': ['AE K T R AH S'],\n",
       " \"ACTRESS'S\": ['AE K T R AH S IH Z'],\n",
       " 'ACTRESSES': ['AE K T R AH S IH Z'],\n",
       " 'ACTS': ['AE K T S', 'AE K S'],\n",
       " 'ACTUAL': ['AE K CH AH W AH L', 'AE K SH AH L'],\n",
       " 'ACTUALITY': ['AE K CH UW AE L AH T IY'],\n",
       " 'ACTUALIZE': ['AE K CH UW AH L AY Z'],\n",
       " 'ACTUALLY': ['AE K CH UW AH L IY', 'AE K CH L IY', 'AE K SH AH L IY'],\n",
       " 'ACTUARIAL': ['AE K CH UW EH R IY AH L'],\n",
       " 'ACTUARIES': ['AE K CH UW EH R IY Z'],\n",
       " 'ACTUARY': ['AE K CH UW EH R IY'],\n",
       " 'ACTUATE': ['AE K CH UW EY T'],\n",
       " 'ACTUATOR': ['AE K T Y UW EY T ER', 'AE K CH UW EY T ER'],\n",
       " 'ACTUATORS': ['AE K T Y UW EY T ER Z', 'AE K CH UW EY T ER Z'],\n",
       " 'ACTUS': ['AE K T AH S'],\n",
       " 'ACUFF': ['AH K AH F'],\n",
       " 'ACUITY': ['AH K Y UW AH T IY'],\n",
       " 'ACUMEN': ['AH K Y UW M AH N'],\n",
       " 'ACUNA': ['AA K UW N AH'],\n",
       " 'ACUPUNCTURE': ['AE K Y UW P AH NG K CH ER'],\n",
       " 'ACURA': ['AE K Y ER AH'],\n",
       " \"ACURA'S\": ['AE K Y ER AH Z'],\n",
       " 'ACURAS': ['AE K Y ER AH Z'],\n",
       " 'ACUSON': ['AE K Y UW S AH N'],\n",
       " 'ACUSTAR': ['AE K Y UW S T AA R'],\n",
       " 'ACUSYST': ['AE K Y UW S IH S T'],\n",
       " 'ACUTE': ['AH K Y UW T'],\n",
       " 'ACUTELY': ['AH K Y UW T L IY'],\n",
       " 'ACUTENESS': ['AH K Y UW T N AH S'],\n",
       " 'ACYCLOVIR': ['AH S IH K L OW V IH R'],\n",
       " 'AD': ['AE D'],\n",
       " \"AD'S\": ['AE D Z'],\n",
       " 'AD-HOC': ['AE D HH AA K'],\n",
       " 'AD-LIB': ['AE D L IH B'],\n",
       " 'AD-NAUSEUM': ['AE D N AO Z IY AH M'],\n",
       " 'ADA': ['EY D AH'],\n",
       " \"ADA'S\": ['EY D AH Z'],\n",
       " 'ADABEL': ['AE D AH B EH L'],\n",
       " 'ADABELLE': ['AE D AH B AH L'],\n",
       " 'ADACHI': ['AA D AA K IY'],\n",
       " 'ADAGE': ['AE D AH JH', 'AE D IH JH'],\n",
       " 'ADAGIO': ['AH D AA ZH IY OW'],\n",
       " 'ADAH': ['AE D AA'],\n",
       " 'ADAIR': ['AH D EH R'],\n",
       " 'ADAIRE': ['AA D EH R'],\n",
       " 'ADAK': ['AH D AE K'],\n",
       " 'ADALAH': ['AA D AA L AH'],\n",
       " 'ADALIA': ['AA D AA L IY AH'],\n",
       " 'ADAM': ['AE D AH M'],\n",
       " \"ADAM'S\": ['AE D AH M Z'],\n",
       " 'ADAMANT': ['AE D AH M AH N T'],\n",
       " 'ADAMANTLY': ['AE D AH M AH N T L IY'],\n",
       " 'ADAMCIK': ['AA D AH M CH IH K'],\n",
       " 'ADAMCZAK': ['AA D AH M CH AE K'],\n",
       " 'ADAMCZYK': ['AA D AH M CH IH K'],\n",
       " 'ADAME': ['AA D AA M IY'],\n",
       " 'ADAMEC': ['AH D AA M IH K'],\n",
       " 'ADAMEK': ['AH D AA M EH K'],\n",
       " 'ADAMES': ['AH D EY M Z'],\n",
       " 'ADAMI': ['AA D AA M IY'],\n",
       " 'ADAMIK': ['AH D AA M IH K'],\n",
       " 'ADAMINA': ['AA D AA M IY N AH'],\n",
       " 'ADAMKUS': ['AE D AH M K AH S'],\n",
       " 'ADAMO': ['AA D AA M OW'],\n",
       " 'ADAMOWICZ': ['AH D AA M AH V IH CH'],\n",
       " 'ADAMS': ['AE D AH M Z'],\n",
       " \"ADAMS'\": ['AE D AH M Z'],\n",
       " \"ADAMS'S\": ['AE D AH M Z IH Z'],\n",
       " 'ADAMSKI': ['AH D AE M S K IY'],\n",
       " 'ADAMSON': ['AE D AH M S AH N'],\n",
       " 'ADAMSTOWN': ['AE D AH M S T AW N'],\n",
       " 'ADAN': ['EY D AH N'],\n",
       " 'ADAPSO': ['AH D AE P S OW'],\n",
       " 'ADAPT': ['AH D AE P T'],\n",
       " 'ADAPTABILITY': ['AH D AE P T AH B IH L AH T IY'],\n",
       " 'ADAPTABLE': ['AH D AE P T AH B AH L'],\n",
       " 'ADAPTAPLEX': ['AH D AE P T AH P L EH K S'],\n",
       " 'ADAPTATION': ['AE D AH P T EY SH AH N'],\n",
       " 'ADAPTATIONS': ['AE D AE P T EY SH AH N Z', 'AE D AH P T EY SH AH N Z'],\n",
       " 'ADAPTEC': ['AH D AE P T EH K'],\n",
       " \"ADAPTEC'S\": ['AH D AE P T EH K S'],\n",
       " 'ADAPTED': ['AH D AE P T AH D', 'AH D AE P T IH D'],\n",
       " 'ADAPTER': ['AH D AE P T ER'],\n",
       " 'ADAPTERS': ['AH D AE P T ER Z'],\n",
       " 'ADAPTING': ['AH D AE P T IH NG'],\n",
       " 'ADAPTIVE': ['AH D AE P T IH V'],\n",
       " 'ADAPTOR': ['AH D AE P T ER'],\n",
       " 'ADAPTORS': ['AH D AE P T ER Z'],\n",
       " 'ADAPTS': ['AH D AE P T S'],\n",
       " 'ADAR': ['AH D AA R'],\n",
       " 'ADARAND': ['AE D AH R AE N D'],\n",
       " 'ADAS': ['EY D AH Z'],\n",
       " 'ADAY': ['AH D EY'],\n",
       " 'ADAZA': ['AH D AA Z AH'],\n",
       " 'ADCOCK': ['AH D K AA K'],\n",
       " 'ADCOX': ['AH D K AA K S'],\n",
       " 'ADD': ['AE D'],\n",
       " 'ADDAIR': ['AH D EH R'],\n",
       " 'ADDAMS': ['AE D AH M Z'],\n",
       " 'ADDED': ['AE D AH D', 'AE D IH D'],\n",
       " 'ADDENDUM': ['AH D EH D AH M'],\n",
       " 'ADDENDUMS': ['AH D EH D AH M Z'],\n",
       " 'ADDEO': ['AA D IY OW'],\n",
       " 'ADDER': ['AE D ER'],\n",
       " 'ADDERLEY': ['AH D ER L IY'],\n",
       " 'ADDICKS': ['AE D IH K S'],\n",
       " 'ADDICT': ['AH D IH K T', 'AE D IH K T'],\n",
       " 'ADDICTED': ['AH D IH K T AH D', 'AH D IH K T IH D'],\n",
       " 'ADDICTING': ['AH D IH K T IH NG'],\n",
       " 'ADDICTION': ['AH D IH K SH AH N'],\n",
       " 'ADDICTIONS': ['AH D IH K SH AH N Z'],\n",
       " 'ADDICTIVE': ['AH D IH K T IH V'],\n",
       " 'ADDICTS': ['AH D IH K T S', 'AE D IH K T S'],\n",
       " 'ADDIDAS': ['AH D IY D AH S'],\n",
       " \"ADDIDAS'\": ['AH D IY D AH S'],\n",
       " \"ADDIDAS'S\": ['AH D IY D AH S IH Z'],\n",
       " 'ADDIDASES': ['AH D IY D AH S IH Z'],\n",
       " 'ADDIE': ['AE D IY'],\n",
       " 'ADDING': ['AE D IH NG'],\n",
       " 'ADDINGTON': ['AE D IH NG T AH N'],\n",
       " 'ADDIS': ['AA D IH S'],\n",
       " 'ADDIS-ABABA': ['AA D IH S AH B AA B AH', 'AA D IY S AH B AA B AH'],\n",
       " 'ADDISON': ['AE D AH S AH N', 'AE D IH S AH N'],\n",
       " \"ADDISON'S\": ['AE D IH S AH N Z'],\n",
       " 'ADDITION': ['AH D IH SH AH N'],\n",
       " 'ADDITIONAL': ['AH D IH SH AH N AH L', 'AH D IH SH N AH L'],\n",
       " 'ADDITIONALLY': ['AH D IH SH AH N AH L IY', 'AH D IH SH N AH L IY'],\n",
       " 'ADDITIONS': ['AH D IH SH AH N Z'],\n",
       " 'ADDITIVE': ['AE D AH T IH V', 'AE D IH T IH V'],\n",
       " 'ADDITIVES': ['AE D AH T IH V Z', 'AE D IH T IH V Z'],\n",
       " 'ADDLE': ['AE D AH L'],\n",
       " 'ADDLED': ['AE D AH L D'],\n",
       " 'ADDLEMAN': ['AE D AH L M AH N'],\n",
       " 'ADDRESS': ['AE D R EH S', 'AH D R EH S'],\n",
       " 'ADDRESSABLE': ['AH D R EH S AH B AH L'],\n",
       " 'ADDRESSED': ['AH D R EH S T'],\n",
       " 'ADDRESSEE': ['AE D R EH S IY'],\n",
       " 'ADDRESSES': ['AE D R EH S IH Z', 'AH D R EH S IH Z'],\n",
       " 'ADDRESSING': ['AH D R EH S IH NG'],\n",
       " 'ADDS': ['AE D Z'],\n",
       " 'ADDUCI': ['AA D UW CH IY'],\n",
       " 'ADDUCT': ['AE D AH K T'],\n",
       " 'ADDWEST': ['AE D W EH S T'],\n",
       " 'ADDY': ['AE D IY'],\n",
       " 'ADDYSTON': ['AE D IY S T AH N'],\n",
       " 'ADE': ['EY D'],\n",
       " 'ADEE': ['AH D IY'],\n",
       " 'ADEL': ['AH D EH L'],\n",
       " 'ADELA': ['AH D EH L AH'],\n",
       " 'ADELAAR': ['AE D AH L AA R'],\n",
       " 'ADELAIDE': ['AE D AH L EY D'],\n",
       " 'ADELANTO': ['AE D AH L AA N T OW'],\n",
       " 'ADELBERT': ['AH D EH L B ER T'],\n",
       " 'ADELE': ['AH D EH L'],\n",
       " \"ADELE'S\": ['AH D EH L Z'],\n",
       " 'ADELINE': ['AE D AH L AY N'],\n",
       " 'ADELIZZI': ['AE D AH L IY Z IY'],\n",
       " 'ADELL': ['AH D EH L'],\n",
       " \"ADELL'S\": ['AH D EH L Z'],\n",
       " 'ADELLE': ['AH D EH L'],\n",
       " 'ADELMAN': ['AE D AH L M AH N', 'EH D AH L M AH N'],\n",
       " 'ADELMANN': ['AE D AH L M AH N'],\n",
       " 'ADELPHA': ['AH D EH L F AH'],\n",
       " 'ADELPHI': ['AH D EH L F IY'],\n",
       " 'ADELPHIA': ['AH D EH L F IY AH'],\n",
       " \"ADELPHIA'S\": ['AH D EH L F IY AH Z'],\n",
       " 'ADELSBERGER': ['AE D IH L Z B ER G ER'],\n",
       " 'ADELSON': ['AE D AH L S AH N'],\n",
       " 'ADELSTEIN': ['AE D AH L S T AY N', 'AE D AH L S T IY N'],\n",
       " 'ADEN': ['EY D AH N'],\n",
       " 'ADENA': ['AE D IH N AH'],\n",
       " 'ADENAUER': ['EY D AH N AW R', 'AE D AH N AW R'],\n",
       " 'ADENINE': ['AE D AH N IY N'],\n",
       " 'ADENOID': ['AE D AH N OY D'],\n",
       " 'ADENOIDS': ['AE D AH N OY D Z'],\n",
       " 'ADENOSCAN': ['AH D EH N AH S K AE N'],\n",
       " 'ADENOSINE': ['AH D EH N AH S IY N'],\n",
       " 'ADENOVIRUS': ['AH D EH N AH V AY R AH S'],\n",
       " 'ADEPT': ['AH D EH P T'],\n",
       " 'ADEQUACY': ['AE D AH K W AH S IY'],\n",
       " 'ADEQUATE': ['AE D AH K W AH T', 'AE D AH K W EY T'],\n",
       " 'ADEQUATELY': ['AE D AH K W AH T L IY', 'AE D AH K W IH T L IY'],\n",
       " 'ADER': ['EY D ER'],\n",
       " 'ADERHOLD': ['AE D ER HH OW L D'],\n",
       " 'ADERHOLT': ['AE D ER HH OW L T'],\n",
       " 'ADERMAN': ['AE D ER M AH N'],\n",
       " 'ADES': ['EY D Z'],\n",
       " 'ADEY': ['EY D IY'],\n",
       " 'ADGER': ['AE JH ER'],\n",
       " 'ADHAM': ['AE D HH AE M'],\n",
       " 'ADHERE': ['AH D HH IH R'],\n",
       " 'ADHERED': ['AE D HH IH R D'],\n",
       " 'ADHERENCE': ['AH D HH IH R AH N S'],\n",
       " 'ADHERENT': ['AH D HH IH R AH N T'],\n",
       " 'ADHERENTS': ['AE D HH IH R AH N T S'],\n",
       " 'ADHERES': ['AH D HH IH R Z'],\n",
       " 'ADHERING': ['AH D HH IH R IH NG'],\n",
       " 'ADHESION': ['AE D HH IY ZH AH N'],\n",
       " 'ADHESIVE': ['AE D HH IY S IH V', 'AH D HH IY S IH V'],\n",
       " 'ADHESIVES': ['AE D HH IY S IH V Z', 'AH D HH IY S IH V Z'],\n",
       " 'ADIA': ['AA D IY AH'],\n",
       " 'ADID': ['AH D IH D'],\n",
       " 'ADIDAS': ['AH D IY D AH S'],\n",
       " \"ADIDAS'S\": ['AH D IY D AH S IH Z'],\n",
       " 'ADIEU': ['AH D UW'],\n",
       " 'ADIN': ['AH D IH N'],\n",
       " 'ADINA': ['AA D IY N AH'],\n",
       " 'ADINE': ['AA D IY N IY'],\n",
       " 'ADINOLFI': ['AA D IY N OW L F IY'],\n",
       " 'ADIOS': ['AA D IY OW S'],\n",
       " 'ADIPOSE': ['AE D AH P OW S'],\n",
       " 'ADIRONDACK': ['AE D ER AA N D AE K'],\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvL61cnvf1TL"
   },
   "source": [
    "## Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tJ6HRZqjFnee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data 739\n"
     ]
    }
   ],
   "source": [
    "#read training data\n",
    "input_data_list = []\n",
    "output_data_list = []\n",
    "with open('wordfun_phonics.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "count = 0    \n",
    "for line in lines:\n",
    "    count += 1\n",
    "    #print(f'line {count}: {line}')    \n",
    "    input_output = re.split('; |, |\\n',line)\n",
    "    #output = input_output[0]\n",
    "    #print(output)\n",
    "    wordCount =0\n",
    "    output_word = ''\n",
    "    for word in input_output:\n",
    "        if wordCount == 0:\n",
    "            output_word = word\n",
    "        else:\n",
    "            if word != '':\n",
    "                input_data_list.append(word)\n",
    "                output_data_list.append(output_word)\n",
    "        \n",
    "        wordCount += 1\n",
    "        \n",
    "print(f'size of training data {len(input_data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RCSquZXcsfVZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z']\n",
      "['\\t', '\\n', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z']\n",
      "Number of samples: 739\n",
      "size of training data 986\n",
      "Number of unique input tokens: 38\n",
      "Number of unique output tokens: 40\n",
      "Max sequence length for inputs: 10\n",
      "Max sequence length for outputs: 12\n"
     ]
    }
   ],
   "source": [
    "## Convert word list to phonemic list\n",
    "\n",
    "input_data_phonics_list = []\n",
    "output_data_phoics_list = []\n",
    "\n",
    "input_phonemes = set()\n",
    "target_phonemes = set() \n",
    "\n",
    "input_data_index = 0\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded_0 = label_encoder.fit_transform(output_data_list)\n",
    "# y_encoded = to_categorical(y_encoded_0)\n",
    "\n",
    "# total_output_classes = len(label_encoder.classes_)\n",
    "# print(f'total_output_classes {total_output_classes}')\n",
    "# print(f'total y_encoded {len(y_encoded)}')\n",
    "\n",
    "# #generate dummy data by duplicate current data\n",
    "# for class_label in label_encoder.classes_:\n",
    "#     for index in range(50):\n",
    "#         input_data_list.append(class_label)\n",
    "#         output_data_list.append(class_label)\n",
    "\n",
    "# y_encoded_0 = label_encoder.fit_transform(output_data_list)\n",
    "# y_encoded = to_categorical(y_encoded_0)\n",
    "\n",
    "# print(f'size of training data  with dummy data {len(input_data_list)}')\n",
    "# print(f'total y_encoded {len(y_encoded)}')\n",
    "\n",
    "for input_data in input_data_list:\n",
    "#     print(input_data.upper())\n",
    "#     print(output_data_list[input_data_index].upper())\n",
    "#     print(y_encoded_0[input_data_index])\n",
    "\n",
    "    seperated_input_word_list = re.split(' ',input_data.upper())\n",
    "    \n",
    "    for single_input_word in seperated_input_word_list:\n",
    "        \n",
    "        for phonics_blending in word_to_phonics(single_input_word):\n",
    "#             print(phonics_blending)\n",
    "            single_word_phoneme_blending_index = []\n",
    "            for phoneme in phonics_blending:\n",
    "                #print(get_phoneme_index(phoneme))\n",
    "                single_word_phoneme_blending_index.append(phoneme)\n",
    "                \n",
    "#             print(output_data_list[input_data_index])\n",
    "            for target_word_phoneme in word_to_phonics(output_data_list[input_data_index].upper()):\n",
    "#                 print(target_word_phoneme)\n",
    "                target_word_phoneme_blending_index = []\n",
    "                target_word_phoneme_blending_index.append('\\t')\n",
    "                for phoneme in target_word_phoneme:\n",
    "                    #print(get_phoneme_index(phoneme))\n",
    "                    target_word_phoneme_blending_index.append(phoneme)\n",
    "                target_word_phoneme_blending_index.append('\\n')\n",
    "                    \n",
    "                input_data_phonics_list.append(single_word_phoneme_blending_index)\n",
    "                output_data_phoics_list.append(target_word_phoneme_blending_index) \n",
    "                \n",
    "                for phoneme in single_word_phoneme_blending_index:\n",
    "#                     print(phoneme)\n",
    "                    if phoneme not in input_phonemes:\n",
    "                        input_phonemes.add(phoneme)\n",
    "                for phoneme in target_word_phoneme_blending_index:\n",
    "                    if phoneme not in target_phonemes:\n",
    "                        target_phonemes.add(phoneme)\n",
    "\n",
    "            #insert space after word\n",
    "            #single_word_phoneme_blending_index.append(get_phoneme_index('SPACE'))\n",
    "            \n",
    "#             #add ending padding\n",
    "#             end_padding = range(SEQUENCE_MAX_LENGTH-len(single_word_phoneme_blending_index))\n",
    "#             for end_padding_index in end_padding:\n",
    "#                 #set the padding valoue to 100\n",
    "#                 single_word_phoneme_blending_index.append(get_phoneme_index('SPACE'))\n",
    "            \n",
    "            \n",
    "            #print(single_word_phoneme_blending_index)\n",
    "            #print(y_encoded[input_data_index])\n",
    "     \n",
    "    \n",
    "    #add ending padding\n",
    "    \n",
    "    input_data_index += 1\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "# print(input_phonemes)\n",
    "# print(target_phonemes)\n",
    "\n",
    "input_phonemes = sorted(list(input_phonemes))\n",
    "target_phonemes = sorted(list(target_phonemes))\n",
    "num_encoder_tokens = len(input_phonemes)\n",
    "num_decoder_tokens = len(target_phonemes)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_data_phonics_list])\n",
    "max_decoder_seq_length = max([len(txt) for txt in output_data_phoics_list])\n",
    "\n",
    "\n",
    "\n",
    "print(input_phonemes)\n",
    "print(target_phonemes)\n",
    "\n",
    "print(\"Number of samples:\", len(input_data_list))\n",
    "print(f'size of training data {len(input_data_phonics_list)}')\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_phonemes)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_phonemes)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_data_phonics_list), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_data_phonics_list), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_data_phonics_list), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_data_phonics_list, output_data_phoics_list)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "#     encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    # decoder_input_data[i, t + 1 :, target_token_index[' ']] = 1.0\n",
    "    # decoder_target_data[i, t:, target_token_index[' ']] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNG-3_kLf1TK"
   },
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9JeWmazVf1TK"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 500  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "#num_samples = 1000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "# data_path = \"yue.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcJMrm4Jf1TO"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "12rOGCImf1TO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 11:57:37.169335: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYCGFp2Pf1TP"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUNihDKJf1TP",
    "outputId": "bbbfb19b-4692-4a87-fb83-6a3728869dfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 11:57:45.952203: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kd/nyt5y4cn4tvbc60yt2dmwn6c0000gp/T/ipykernel_57606/2810769409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rmsprop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bi-directional_lstm_translator/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "# model.save(\"phonics2phonics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"phonics2phonics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsWCCMSaf1TQ"
   },
   "source": [
    "## Run inference (sampling)\n",
    "\n",
    "1. encode input and retrieve initial decoder state\n",
    "2. run one step of decoder with this initial state\n",
    "and a \"start of sequence\" token as target.\n",
    "Output will be the next target token.\n",
    "3. Repeat with the current target token and current states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qENTmoLqf1TQ"
   },
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"phonics2phonics\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "#         if (sampled_char !='\\t' or sampled_char != '\\n'):\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfR3cDMnf1TQ"
   },
   "source": [
    "You can now generate decoded sentences as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EqByMn1lG4Zv",
    "outputId": "eb03c640-89dc-4826-c967-a03b0bf16e06"
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    i=0\n",
    "    input_data = input(\"Enter your word: \")\n",
    "    \n",
    "    phoneme_blendings = word_to_phonics(input_data.upper())\n",
    "    \n",
    "    for phoneme_blending in phoneme_blendings:\n",
    " \n",
    "\n",
    " \n",
    "    #     for phonics_blending in word_to_phonics(input_data.upper()):\n",
    "        encoder_input_data_1 = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "    #     for t, char in enumerate(input_data.upper()):\n",
    "\n",
    "        input_word_phoneme = ''\n",
    "        for t in range(len(phoneme_blending)):\n",
    "           \n",
    "            char = phoneme_blending[t]\n",
    "            input_word_phoneme += char + ' '\n",
    "    #         print(char)\n",
    "    #         print(t)\n",
    "            encoder_input_data_1[i, t, input_token_index[char]] = 1.0\n",
    "    #     encoder_input_data_1[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "        # print(encoder_input_data_1[0,1])\n",
    "        input_seq = encoder_input_data_1[0 :  1]\n",
    "        # print(input_seq)\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        # decoded_sentence = decode_sequence(encoder_input_data_1[0:1])\n",
    "        # print(\"-\")\n",
    "        #print(\"Input sentence:\", input_texts[seq_index])\n",
    "    #     decoded_output = re.split('\\t|, |\\n',decoded_sentence)\n",
    "\n",
    "    #     decoded_sentence = ''\n",
    "    #     for ouput in decoded_output:\n",
    "    #         if output != '':\n",
    "    #             decoded_sentence = decoded_sentence + output\n",
    "        print(\"Input word phonics blending:\", input_word_phoneme)\n",
    "        print(\"Decoded word phonics blending:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpJAypJlf1TR",
    "outputId": "af68e87d-92bb-4f12-bb6c-1ed907c840dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ['R', 'EH', 'D']\n",
      "Decoded sentence: R EH D \n",
      " \n",
      "-\n",
      "Input sentence: ['G', 'L', 'AE', 'D']\n",
      "Decoded sentence: R EH D \n",
      " \n",
      "-\n",
      "Input sentence: ['R', 'EH', 'D']\n",
      "Decoded sentence: R EH D \n",
      " \n",
      "-\n",
      "Input sentence: ['R', 'IY', 'D']\n",
      "Decoded sentence: R AE B AH T \n",
      " \n",
      "-\n",
      "Input sentence: ['R', 'AE', 'T']\n",
      "Decoded sentence: R EH D \n",
      " \n",
      "-\n",
      "Input sentence: ['G', 'R', 'IY', 'N']\n",
      "Decoded sentence: G R IY N \n",
      " \n",
      "-\n",
      "Input sentence: ['K', 'L', 'IY', 'N']\n",
      "Decoded sentence: G R IY N \n",
      " \n",
      "-\n",
      "Input sentence: ['G', 'R', 'EY', 'T']\n",
      "Decoded sentence: G R IY N \n",
      " \n",
      "-\n",
      "Input sentence: ['B', 'L', 'UW']\n",
      "Decoded sentence: B L UW \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'EH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AY', 'T']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AY', 'T']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AY', 'T']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AY', 'T']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AY']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AY']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AY']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AY']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AH', 'T']\n",
      "Decoded sentence: W AA S P \n",
      " \n",
      "-\n",
      "Input sentence: ['W', 'AH', 'T']\n",
      "Decoded sentence: W AA S P \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AH', 'T']\n",
      "Decoded sentence: W AA S P \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'W', 'AH', 'T']\n",
      "Decoded sentence: W AA S P \n",
      " \n",
      "-\n",
      "Input sentence: ['L', 'AY', 'K']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['L', 'AY', 'K']\n",
      "Decoded sentence: HH W AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['B', 'L', 'AE', 'K']\n",
      "Decoded sentence: B L AE K \n",
      " \n",
      "-\n",
      "Input sentence: ['B', 'R', 'EY', 'K']\n",
      "Decoded sentence: B L AE K \n",
      " \n",
      "-\n",
      "Input sentence: ['AO', 'R', 'AH', 'N', 'JH']\n",
      "Decoded sentence: AO R AH N JH \n",
      "-\n",
      "Input sentence: ['AO', 'R', 'AH', 'N', 'JH']\n",
      "Decoded sentence: AO R AH N JH \n",
      "-\n",
      "Input sentence: ['AO', 'R', 'IH', 'N', 'JH']\n",
      "Decoded sentence: AO R AH N JH \n",
      "-\n",
      "Input sentence: ['AO', 'R', 'IH', 'N', 'JH']\n",
      "Decoded sentence: AO R AH N JH \n",
      "-\n",
      "Input sentence: ['R', 'EY', 'N']\n",
      "Decoded sentence: AO R IH N JH \n",
      "-\n",
      "Input sentence: ['R', 'EY', 'N']\n",
      "Decoded sentence: AO R IH N JH \n",
      "-\n",
      "Input sentence: ['P', 'ER', 'P', 'AH', 'L']\n",
      "Decoded sentence: P ER P AH L \n",
      " \n",
      "-\n",
      "Input sentence: ['K', 'AH', 'L', 'IH', 'P', 'S', 'OW']\n",
      "Decoded sentence: P ER P AH L \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'EH', 'L', 'P']\n",
      "Decoded sentence: HH IH L \n",
      " \n",
      "-\n",
      "Input sentence: ['Y', 'EH', 'L', 'OW']\n",
      "Decoded sentence: Y EH L OW \n",
      " \n",
      "-\n",
      "Input sentence: ['P', 'IH', 'NG', 'K']\n",
      "Decoded sentence: P IH NG K \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'P', 'IY', 'K']\n",
      "Decoded sentence: P IH NG K \n",
      " \n",
      "-\n",
      "Input sentence: ['P', 'IH', 'K']\n",
      "Decoded sentence: P IH NG K \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'ER', 'K', 'AH', 'L']\n",
      "Decoded sentence: S ER K AH L \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'K', 'W', 'EH', 'R']\n",
      "Decoded sentence: S K W EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'K', 'R', 'IY', 'N']\n",
      "Decoded sentence: S K W EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'K', 'R', 'UW']\n",
      "Decoded sentence: S K W EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'K', 'W', 'ER', 'AH', 'L']\n",
      "Decoded sentence: S K W EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'T', 'AA', 'R']\n",
      "Decoded sentence: S T AA R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'T', 'AA', 'R', 'T']\n",
      "Decoded sentence: S T AA R \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'T', 'AA', 'P']\n",
      "Decoded sentence: D AO G \n",
      " \n",
      "-\n",
      "Input sentence: ['T', 'R', 'AY', 'AE', 'NG', 'G', 'AH', 'L']\n",
      "Decoded sentence: T R AY AE NG \n",
      "-\n",
      "Input sentence: ['CH', 'AH', 'K']\n",
      "Decoded sentence: T R AY AE NG \n",
      "-\n",
      "Input sentence: ['R', 'EH', 'K', 'T', 'AE', 'NG', 'G', 'AH', 'L']\n",
      "Decoded sentence: R EH K T AE NG \n",
      "-\n",
      "Input sentence: ['R', 'EY', 'T']\n",
      "Decoded sentence: R EH K T AE NG \n",
      "-\n",
      "Input sentence: ['R', 'EH', 'D', 'IY']\n",
      "Decoded sentence: R EH D IH NG \n",
      "-\n",
      "Input sentence: ['T', 'UW']\n",
      "Decoded sentence: HH AE M S T ER \n",
      "-\n",
      "Input sentence: ['T', 'IH']\n",
      "Decoded sentence: V EH JH T AH \n",
      "-\n",
      "Input sentence: ['T', 'AH']\n",
      "Decoded sentence: V EH JH T AH \n",
      "-\n",
      "Input sentence: ['G', 'OW']\n",
      "Decoded sentence: G OW \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AA', 'R', 'T']\n",
      "Decoded sentence: HH AA R T \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'EH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AA', 'T']\n",
      "Decoded sentence: HH AA T \n",
      " \n",
      "-\n",
      "Input sentence: ['AE', 'R', 'OW']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['AE', 'R', 'OW']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'OW']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'OW']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'AH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'EH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['HH', 'EH', 'L', 'OW']\n",
      "Decoded sentence: HH EH R \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'AH']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'AH']\n",
      "Decoded sentence: AE R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['IH', 'R', 'AH']\n",
      "Decoded sentence: EH R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['IH', 'R', 'AH']\n",
      "Decoded sentence: EH R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'IH', 'K']\n",
      "Decoded sentence: EH R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['EH', 'R', 'IH', 'K']\n",
      "Decoded sentence: EH R OW \n",
      " \n",
      "-\n",
      "Input sentence: ['K', 'R', 'AO', 'S']\n",
      "Decoded sentence: K R AO S \n",
      " \n",
      "-\n",
      "Input sentence: ['K', 'L', 'OW', 'S']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['K', 'L', 'OW', 'Z']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['D', 'AY', 'M', 'AH', 'N', 'D']\n",
      "Decoded sentence: D AY M AH N D \n",
      "-\n",
      "Input sentence: ['D', 'AY', 'M', 'AH', 'N', 'D', 'Z']\n",
      "Decoded sentence: D AY M AH N D \n",
      "-\n",
      "Input sentence: ['L', 'EH', 'F', 'T']\n",
      "Decoded sentence: L EH F T \n",
      " \n",
      "-\n",
      "Input sentence: ['L', 'AE', 'F']\n",
      "Decoded sentence: L EH F T \n",
      " \n",
      "-\n",
      "Input sentence: ['L', 'AE', 'P']\n",
      "Decoded sentence: L EH F T \n",
      " \n",
      "-\n",
      "Input sentence: ['R', 'AY', 'T']\n",
      "Decoded sentence: R AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['R', 'AY', 'T', 'S']\n",
      "Decoded sentence: R AY T \n",
      " \n",
      "-\n",
      "Input sentence: ['AH', 'P']\n",
      "Decoded sentence: SH ER T \n",
      " \n",
      "-\n",
      "Input sentence: ['AE', 'P']\n",
      "Decoded sentence: AH P \n",
      " \n",
      "-\n",
      "Input sentence: ['P', 'AE', 'T']\n",
      "Decoded sentence: AH P \n",
      " \n",
      "-\n",
      "Input sentence: ['D', 'AW', 'N']\n",
      "Decoded sentence: D AW N \n",
      " \n",
      "-\n",
      "Input sentence: ['D', 'AH', 'N']\n",
      "Decoded sentence: D AW N \n",
      " \n",
      "-\n",
      "Input sentence: ['F', 'OW', 'N']\n",
      "Decoded sentence: D AW N \n",
      " \n",
      "-\n",
      "Input sentence: ['JH', 'AA', 'N']\n",
      "Decoded sentence: D AW N \n",
      " \n",
      "-\n",
      "Input sentence: ['S', 'T', 'AA', 'P']\n",
      "Decoded sentence: D AO G \n",
      " \n",
      "-\n",
      "Input sentence: ['F', 'OW', 'N']\n",
      "Decoded sentence: D AW N \n",
      " \n",
      "-\n",
      "Input sentence: ['F', 'AO', 'R', 'W', 'ER', 'D', 'Z']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['F', 'AO', 'R', 'W', 'ER', 'D']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['K', 'L', 'OW', 'S']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['K', 'L', 'OW', 'Z']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['F', 'L', 'AW', 'ER', 'Z']\n",
      "Decoded sentence: F AO R W ER D \n",
      "-\n",
      "Input sentence: ['F', 'AA', 'L', 'OW']\n",
      "Decoded sentence: F AO R W ER D \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    # print(encoder_input_data[seq_index : seq_index + 1])\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "#     print(input_seq)\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_data_phonics_list[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_seq2seq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
